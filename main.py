# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/naufalmuafi/bank-customer-churn-prediction/blob/master/main.ipynb

# **Bank Customer Churn Prediction**

Predictive Analytics Project with *Bank Customer Churn Prediction Datasets*

Naufal Mu'afi<br>
naufalmuafi@mail.ugm.ac.id

---


<b>*Project Executive Summary*</b>

In this project, I'll discuss an `economics` or `business` issue that specifically analyzes the prediction of Bank Customer Churn. Determining whether the customer is more likely to stay or exit from the Bank. This project aims to compare various algorithms to evaluate the best model for this prediction.

First thing first, we can import some necessary libraries that we'll use in this project.
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

# %matplotlib inline

from sklearn.utils import resample
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

"""## 1. Data Loading/Data Wrangling
---

Dataset Information:

Type | Information
--- | ---
Source | [Kaggle Dataset : Bank Customer Churn Prediction](https://www.kaggle.com/datasets/shantanudhakadd/bank-customer-churn-prediction/data)
License | Other
Category | `Economy`, `Business`, `Finance`, `Banking`
Usability Rating | 9.71
File Type and Size | CSV (268 kb)

we can view the first 5 rows data of the dataset using `head()` method.
"""

churn = pd.read_csv("./data/Churn_Modelling.csv")
churn.head()

"""## 2. Exploratory Data Analysis (EDA)
---

### 2.1 Assesing and Cleaning Data

#### 2.1.1 Variable Description

This dataset `Churn_Modelling.csv` is originated from a U.S. bank that contains information about this particular customer will leave bank or not.
"""

churn.shape
print(f"The dataset has {churn.shape[0]} rows, and {churn.shape[1]} columns")

churn.info()

churn.describe()

"""`Churn_Modelling.csv` dataset file has 10,000 rows and 14 columns. This means the dataset contains information for 10.000 bank customers, including 14 various details such as Gender, Age, Geography, and many more. The dataset comprises 9 columns of `int64` data type, 2 columns of `float64` data type, and 3 columns of `object` data type. It's not possible to determine whether the data represents numerical or categorical features based solely on the data type. Initially, we identify 3 dummy features in the dataset:  `RowNumber`, `CustomerId`, and `Surname`. Upon further examination, we classify the dataset as having 4 numerical features, and 7 categorical features.


Explanation of 14 columns or feature from the `Churn_Modelling.csv` dataset:

1. `RowNumber` - Indicates the row number.

2. `CustomerId` - Each customer has a unique ID stored in this feature.

3. `Surname` - The Bank customer's surname.

4. `CreditScore` - Explains how the Bank Customer's Credit Score is rated. In this dataset, the value ranges from 350 to 850.

5. `Geography` - Customer demographics based on the country.

6. `Gender` - Gender of the Bank Customer.

7. `Age` - Bank Customer Age, ranging from 18 to 92 years old.

8. `Tenure` - Duration a customer has been associated with or held an account with the bank. In this dataset, the distribution of this feature ranges from 0 to 10 years.

9. `Balance` - Refers to the average amount of money held in a customer's account. It's a continuous feature with minimum balance is $0, and the maximum balance is $250898.09,

10. `NumOfProducts` - The variety or count of financial prodcuts or services that a customer holds with the Bank. In this dataset, the bank has a total of 4 products.

11. `HasCrCard` - Binary Classification of whether a Bank Customer has a credit card or not.

12. `IsActiveMember` - a feature for customer classification; it indicates whether the customer is still active member in the bank or has become a passive member.

13. `EstimatedSalary` - an Approximation or prediction of a customer's individual income in a month. In this dataset, incomes range from $11.58 to $199992.48.

14. `Exited` - The target or output that provides various details to decide either a customer is more likely to stay in the bank or has exited from the bank.
"""

print(f"\nNumber of duplications: {churn.duplicated().sum()}")

"""This dataset has also been validated as free from duplicate data

#### 2.1.2 Variable Distribution Classification

As explained earlier, there're 3 dummy features. We can drop theese unnecessary/dummy features from the dataframe.
"""

churn.drop(['RowNumber', 'CustomerId', 'Surname'], axis=1, inplace=True)
churn.head()

"""Next, we can classify the features into either numerical or categorical features."""

numerical_features = []
categorical_features = []

nfeatures = len(churn.nunique())

for i in range(nfeatures):
  feature_uniqueness = churn.nunique()

  if feature_uniqueness.values[i] <= 25:
    categorical_features.append(feature_uniqueness.index[i])
  else:
    numerical_features.append(feature_uniqueness.index[i])

print(f"Numerical Features: {numerical_features}")
print(f"Categorical Features: {categorical_features}")

"""#### 2.1.3 Handle Missing Value"""

churn.isna().sum()

"""The dataset is free from missing values, so we can proceed to the next step.

#### 2.1.4. Handle The Outliers
"""

nrows = 2
ncols = 2

fig, ax = plt.subplots(nrows, ncols, figsize=(20, 15))

for row in range(nrows):
  for col in range(ncols):
    column = numerical_features[row*ncols + col]

    sns.boxplot(x=churn[column], ax=ax[row, col])

"""We can observe that some CreditScore and Age features exhibit outliers. To address this, we can drop the outliers using Interquartile Range (IQR) measures. The IQR measures the spread of the middle half of the data, calculated as the difference between the third and the first quartile of the data."""

Q1 = churn[numerical_features].quantile(0.25)
Q3 = churn[numerical_features].quantile(0.75)
IQR = Q3-Q1

# # creating a mask for outliers
outlier_mask = ((churn[numerical_features] < (Q1 - 1.5*IQR)) | (churn[numerical_features] > (Q3 + 1.5*IQR))).any(axis=1)

# #filtering out rows with outliers
churn = churn[~outlier_mask]

churn.shape

nrows = 2
ncols = 2

fig, ax = plt.subplots(nrows, ncols, figsize=(20, 15))

for row in range(nrows):
  for col in range(ncols):
    column = numerical_features[row*ncols + col]

    sns.boxplot(x=churn[column], ax=ax[row, col])

"""### 2.2 Univariate Analysis for Categorical Features

#### 2.2.1 Geography Feature
"""

index = 0
feature = categorical_features[index]
count = churn[feature].value_counts()
percent = 100*churn[feature].value_counts(normalize=True)
df = pd.DataFrame({'sample total':count, 'percentage':percent.round(1)})
print(df)

# Plotting the bar chart
ax = count.plot(kind='bar', title=feature)

# Adding labels
plt.xlabel(feature)
plt.ylabel('Count')
plt.title(f'{feature} Distribution')
plt.xticks(rotation=45)

# Adding data labels on each bar
for i, v in enumerate(count):
    ax.text(i, v + 0.5, f'{percent.iloc[i]:.1f}%', ha='center', va='bottom')

plt.show()

"""Based on the distribution, we can conclude that the majority of customers come from France, followed by Germany and Spain.

#### 2.2.2 Gender Feature
"""

index = 1
feature = categorical_features[index]
count = churn[feature].value_counts()
percent = 100*churn[feature].value_counts(normalize=True)
df = pd.DataFrame({'sample total':count, 'percentage':percent.round(1)})
print(df)

# Plotting the bar chart
ax = count.plot(kind='bar', title=feature)

# Adding labels
plt.xlabel(feature)
plt.ylabel('Count')
plt.title(f'{feature} Distribution')
plt.xticks(rotation=45)

# Adding data labels on each bar
for i, v in enumerate(count):
    ax.text(i, v + 0.5, f'{percent.iloc[i]:.1f}%', ha='center', va='bottom')

plt.show()

"""Analyzing the distribution, it can be inferred that the predominant gender among customers is male, accounting for 54.6%, while females constitute 45.4%.

#### 2.2.3 Tenure Feature
"""

index = 2
feature = categorical_features[index]
count = churn[feature].value_counts()
percent = 100*churn[feature].value_counts(normalize=True)
df = pd.DataFrame({'sample total':count, 'percentage':percent.round(1)})
print(df)

# Plotting the bar chart
ax = count.plot(kind='bar', title=feature)

# Adding labels
plt.xlabel(feature)
plt.ylabel('Count')
plt.title(f'{feature} Distribution')
plt.xticks(rotation=45)

# Adding data labels on each bar
for i, v in enumerate(count):
    ax.text(i, v + 0.5, f'{percent.iloc[i]:.1f}%', ha='center', va='bottom')

plt.show()

"""Upon analyzing the distribution, it can be deduced that customer tenure in the bank is diverse. Approximately 10.5% of customers have a tenure of 2 years, and another 10.4% have been with the bank for 7 to 8 years. New and long-term customers constitute the minority in this distribution.

#### 2.2.4 Num of Product Feature
"""

index = 3
feature = categorical_features[index]
count = churn[feature].value_counts()
percent = 100*churn[feature].value_counts(normalize=True)
df = pd.DataFrame({'sample total':count, 'percentage':percent.round(1)})
print(df)

# Plotting the bar chart
ax = count.plot(kind='bar', title=feature)

# Adding labels
plt.xlabel(feature)
plt.ylabel('Count')
plt.title(f'{feature} Distribution')
plt.xticks(rotation=45)

# Adding data labels on each bar
for i, v in enumerate(count):
    ax.text(i, v + 0.5, f'{percent.iloc[i]:.1f}%', ha='center', va='bottom')

plt.show()

"""Based on the distribution, we can conclude that the majority of customers have 1 to 2 Products or services in the bank.

#### 2.2.5 Has Credit Card Feature
"""

index = 4
feature = categorical_features[index]
count = churn[feature].value_counts()
percent = 100*churn[feature].value_counts(normalize=True)
df = pd.DataFrame({'sample total':count, 'percentage':percent.round(1)})
print(df)

# Plotting the bar chart
ax = count.plot(kind='bar', title=feature)

# Adding labels
plt.xlabel(feature)
plt.ylabel('Count')
plt.title(f'{feature} Distribution')
plt.xticks(rotation=45)

# Adding data labels on each bar
for i, v in enumerate(count):
    ax.text(i, v + 0.5, f'{percent.iloc[i]:.1f}%', ha='center', va='bottom')

plt.show()

"""Analyzing the plot, it can be inferred that the predominant distribution among customers is has credit card, accounting for 70.5%, while customers who doesn't have Credit Card constitute 29.5%.

#### 2.2.6 Is Active Member Feature
"""

index = 5
feature = categorical_features[index]
count = churn[feature].value_counts()
percent = 100*churn[feature].value_counts(normalize=True)
df = pd.DataFrame({'sample total':count, 'percentage':percent.round(1)})
print(df)

# Plotting the bar chart
ax = count.plot(kind='bar', title=feature)

# Adding labels
plt.xlabel(feature)
plt.ylabel('Count')
plt.title(f'{feature} Distribution')
plt.xticks(rotation=45)

# Adding data labels on each bar
for i, v in enumerate(count):
    ax.text(i, v + 0.5, f'{percent.iloc[i]:.1f}%', ha='center', va='bottom')

plt.show()

"""Analyzing the plot, it can be deduced that 50.4% of customers are active, while the remaining 49.6% are categorized as passive members.

#### 2.2.7 Exited Feature
"""

index = 6
feature = categorical_features[index]
count = churn[feature].value_counts()
percent = 100*churn[feature].value_counts(normalize=True)
df = pd.DataFrame({'sample total':count, 'percentage':percent.round(1)})
print(df)

# Plotting the bar chart
ax = count.plot(kind='bar', title=feature)

# Adding labels
plt.xlabel(feature)
plt.ylabel('Count')
plt.title(f'{feature} Distribution')
plt.xticks(rotation=45)

# Adding data labels on each bar
for i, v in enumerate(count):
    ax.text(i, v + 0.5, f'{percent.iloc[i]:.1f}%', ha='center', va='bottom')

plt.show()

"""Considering the distribution, it can be concluded that the majority of customers are more likely to stay than to exit from the bank. However, this leads to imbalanced data for our target parameter.

### 2.3 Univariate Analysis for Numerical Features
"""

churn[numerical_features].hist(bins=50, figsize=(20,15))
plt.show()

"""From the plot, it can be inferred that the features 'CreditScore,' 'Age,' and 'Balance' exhibit characteristics that make them suitable for categorization as normally distributed. Despite a spike in the 'Balance' feature at 0, suggesting a concentration of values, the overall distribution appears to follow a normal pattern. On the other hand, the 'EstimatedSalary' feature does not show a clear correlation or relationship within the data. This observation indicates that 'EstimatedSalary' may not follow a distinct distribution pattern or may lack a discernible trend.

### 2.4 Multivariate Analysis

#### 2.4.1 Categorical Features
"""

cat_features = categorical_features.copy()
cat_features.remove('Exited')

for col in cat_features:
  sns.catplot(x=col, y='Exited', kind='bar', dodge=False, height=4, aspect=3, data=churn, palette="Set3")
  plt.title(f"Average 'Exited' Relative to {col}")

"""- Regarding the `Geography` feature, customers from Germany are more likely to stay in the bank compared to those from France or Spain.
- Concerning the `Gender` feature, females are more likely to stay in the bank compared to males.
- In both the `Tenure` and `HasCrCard` features, the average durations tend to be similar.
- Interestingly, in the `IsActiveMember` feature, active members are more likely to exit from the bank.

In conclusion, it can be inferred that categorical features have a relatively minor impact on customer decisions to stay or exit from the bank.

#### 2.4.1 Numerical Features

##### Pair Plot
"""

# Observing relationships between numerical features
sns.pairplot(churn, diag_kind = 'kde')

"""##### Corresponding Targets for Each Numerical Feature"""

nrows = 2
ncols = 2

fig, ax = plt.subplots(nrows, ncols, figsize=(20, 15))

for row in range(nrows):
  for col in range(ncols):
    column = numerical_features[row*ncols + col]

    sns.barplot(x=churn['Exited'], y=churn[column], ax=ax[row, col], palette='Set2')

"""We can observe that there's a low or tend to be no correlation with the `Exited` feature. Consequently, we can conclude that numerical features also have a relatively minor impact on customer decisions to stay or exit from the bank.

## 3. Data Preparation
---

### 3.1 Handle the Imbalance Data with Resample
"""

churn['Exited'].value_counts().to_frame()

"""As explained in the univariate analysis, the target feature exhibits imbalanced data. To address this imbalance, we can employ the oversampling method, which involves adding n_samples to the minority data, thereby achieving a more balanced target data."""

target_majority = churn[churn['Exited']==0]
target_minority = churn[churn['Exited']==1]

oversampling = resample(
  target_minority,
  n_samples=7677,
  replace=True,
  random_state=42
)

churn = pd.concat([oversampling, target_majority])

churn['Exited'].value_counts().to_frame()

"""### 3.2 Category Feature Encoding"""

churn = pd.get_dummies(churn, columns=['Geography', 'Gender'], drop_first=True, dtype=np.int8)
churn.head()

"""Afterwards, the categorical and nominal features need to be encoded into 1s or 0s. For the encoding process, the `get_dummies()` method is utilized with the `drop_first` parameter to prevent multicollinearity in the data.

### 3.3 Correlation Analysis
"""

plt.figure(figsize=(12,10))
corr_matrix = churn.corr()

sns.heatmap(data=corr_matrix,
            xticklabels=corr_matrix.columns,
            yticklabels=corr_matrix.columns,
            annot=True,
            cmap='coolwarm',
            linewidths=0.5)

plt.title('Correlation Matrix of The Dataset', fontsize=20)
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)
plt.show()

"""As we can observe, both the categorical and numerical features exhibit a low correlation with the `Exited` feature.

### 3.4 Train Test Split
"""

X = churn.drop(['Exited'], axis=1)
y = churn['Exited']

X_train, X_test, y_train, y_test = train_test_split(X,
                                                    y,
                                                    stratify=y,
                                                    test_size=0.2,
                                                    random_state=123,
                                                    shuffle=True)

print(f'Total # of sample in whole dataset: {len(X)}')
print(f'Total # of sample in train dataset: {len(X_train)}')
print(f'Total # of sample in test dataset: {len(X_test)}')

"""The dataset is divided into training and test datasets, with the training dataset comprising 80% of the data and the test dataset comprising 20%.

### 3.5 Feature Scaling
"""

scaler = StandardScaler()
features = list(X_train.columns)

for col in features:
    X_train[col] = scaler.fit_transform(X_train[col].to_numpy().reshape(-1,1))
    X_test[col] = scaler.transform(X_test[col].to_numpy().reshape(-1,1))

X_train[numerical_features].describe().round(4)

X_train.head()

"""By scaling or standardizing the features to a range of 0 to 1, the numerical features are brought to the same scale, which positively impacts computation and enables the model to run faster.

## 4. Model Development
---

In this section, we will develop four models aiming for the best performance in terms of accuracy. Prior to training the models, the Grid Search method is employed to tune the hyperparameters for each model.

### 4.1 K-Nearest Neighbourhood Algorithm
"""

knn_model = KNeighborsClassifier()
knn_parameters = {
  'n_neighbors' : [5, 10, 15]
}
grid_search = GridSearchCV(knn_model, knn_parameters)
grid_search.fit(X_train, y_train)
print(grid_search.best_params_)

knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train, y_train)

knn_pred = knn.predict(X_test)
knn_acc = accuracy_score(y_test, knn_pred)

print(f"Accuracy of KNN {knn_acc*100:.2f}%")

knn_report = classification_report(y_test, knn_pred, output_dict=True, target_names=['Stay', 'Exit'])
pd.DataFrame(knn_report).transpose()

"""First, the KNN Algorithm achieved an accuracy score of 81.24%, and the precision, recall, and f1-score values indicate satisfactory performance.

### 4.2 Logistic Regression Algorithm
"""

lr_model = LogisticRegression()
lr_parameters = {
  'C'         : [1000, 10000, 100000],
  'max_iter'  : [100, 250, 500]
}
grid_search = GridSearchCV(lr_model, lr_parameters)
grid_search.fit(X_train, y_train)
print(grid_search.best_params_)

lr = LogisticRegression(
  C=1000,
  max_iter=100,
  random_state=123
)
lr.fit(X_train, y_train)

lr_pred = lr.predict(X_test)
lr_acc = accuracy_score(y_test, lr_pred)

print(f"Accuracy of lr {lr_acc*100:.2f}%")

lr_report = classification_report(y_test, lr_pred, output_dict=True, target_names=['Stay', 'Exit'])
pd.DataFrame(lr_report).transpose()

"""Second, the Logistic Regression Algorithm achieved an accuracy score of 72.48%, which is lower than the first model. The precision, recall, and f1-score values also indicate a comparatively lower performance.

### 4.3 Support Vector Classifier Algorithm
"""

svc_model = SVC()
svc_parameters = {
  'C'       : [1.0, 2.0, 3.0, 4.0, 5.0],
  'gamma'   : [0.1, 1, 10, 100],
  'kernel'  : ['rbf']
}
grid_search = GridSearchCV(svc_model, svc_parameters)
grid_search.fit(X_train, y_train)
print(grid_search.best_params_)

svc = SVC(
  C=1.0,
  gamma=100,
  kernel='rbf'
)
svc.fit(X_train, y_train)

svc_pred = svc.predict(X_test)
svc_acc = accuracy_score(y_test, svc_pred)

print(f"Accuracy of svc {svc_acc*100:.2f}%")

svc_report = classification_report(y_test, svc_pred, output_dict=True, target_names=['Stay', 'Exit'])
pd.DataFrame(svc_report).transpose()

"""Moving on, the Support Vector Classifier (SVC) Algorithm achieved an impressive accuracy score of 97.62%, surpassing both the first and second models. The precision, recall, and f1-score values further indicate significantly higher performance.

### 4.4 Random Forest Algorithm
"""

rf_model = RandomForestClassifier()
rf_parameters = {
  'n_estimators'  : [50, 100, 150, 200],
  'max_depth'     : [10, 15, 20]
}
grid_search = GridSearchCV(rf_model, rf_parameters)
grid_search.fit(X_train, y_train)
print(grid_search.best_params_)

rf = RandomForestClassifier(
  n_estimators=200,
  max_depth=20,
  random_state=123,
  n_jobs=-1
)
rf.fit(X_train, y_train)

rf_pred = rf.predict(X_test)
rf_acc = accuracy_score(y_test, rf_pred)

print(f"Accuracy of rf {rf_acc*100:.2f}%")

rf_report = classification_report(y_test, rf_pred, output_dict=True, target_names=['Stay', 'Exit'])
pd.DataFrame(rf_report).transpose()

"""In the final model developed, the Random Forest Algorithm achieved an accuracy score of 94.89%. The precision, recall, and f1-score values consistently indicate high performance across the evaluation metrics.

## 5. Model Evaluation
---

### 5.1 Confusion Matrix
"""

knn_cm  = confusion_matrix(y_test, knn_pred)
lr_cm   = confusion_matrix(y_test, lr_pred)
svc_cm  = confusion_matrix(y_test, svc_pred)
rf_cm   = confusion_matrix(y_test, rf_pred)

plt.figure(figsize=(20, 15))

plt.subplot(2, 2, 1)
sns.heatmap(knn_cm, annot=True, cmap='Blues', fmt='d', cbar=False)
plt.title('KNN Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')

plt.subplot(2, 2, 2)
sns.heatmap(lr_cm, annot=True, cmap='Greens', fmt='d', cbar=False)
plt.title('Logistic Regression Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')

plt.subplot(2, 2, 3)
sns.heatmap(svc_cm, annot=True, cmap='Oranges', fmt='d', cbar=False)
plt.title('SVC Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')

plt.subplot(2, 2, 4)
sns.heatmap(rf_cm, annot=True, cmap='Reds', fmt='d', cbar=False)
plt.title('Random Forest Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')

plt.tight_layout()
plt.show()

"""Based on the confusion matrix, it can be concluded that all the models we have developed perform well in predicting true positive and true negative values.

### 5.2 Model Comparison
"""

classifiers = ['KNN', 'LogisticRegression', 'SVC', 'RandomForest']
accuracies = [knn_acc, lr_acc, svc_acc, rf_acc]

plt.figure(figsize=(8, 6))
plt.bar(classifiers, accuracies, color=['blue', 'green', 'orange', 'red'])
plt.title('Comparison of Models Accuracies')
plt.xlabel('Classifier')
plt.ylabel('Accuracy')
plt.ylim(0, 1)
for i, acc in enumerate(accuracies):
    plt.text(i, acc + 0.01, f'{acc:.2f}', ha='center')

plt.show()

acc = pd.DataFrame(columns=['train', 'test'], index=['KNN', 'LR', 'SVC', 'RF'])
model_dict = {'KNN': knn, 'LR':lr, 'SVC':svc, 'RF':rf}

# calculate MSE for each algorithm in train and test dataset
for name, model in model_dict.items():
  acc.loc[name, 'train'] = accuracy_score(y_true=y_train, y_pred=model.predict(X_train))*100
  acc.loc[name, 'test'] = accuracy_score(y_true=y_test, y_pred=model.predict(X_test))*100

acc

fig, ax = plt.subplots()
acc.sort_values(by='test', ascending=True).plot(kind='barh', ax=ax, zorder=3)
ax.grid(zorder=0)

"""Upon examination of the visuals, it is evident that the SVC algorithm provides the highest accuracy in both the training and test sets. Therefore, the SVC algorithm is chosen as the best model for predicting the churn of bank customers.

## 6. Model Prediction
---

To test the model, we can generate predictions using some data from the dataset.
"""

prediction = X_test.iloc[:5].copy()
pred_dict = {'y_true':y_test[:5]}

for name, model in model_dict.items():
  pred_dict['prediction_'+name] = model.predict(prediction).round(1)

pd.DataFrame(pred_dict)

"""The prediction results with SVC and Random Forest Algorithm yield the best approximate results. Subsequently, we can create a utility function to assess the model with parameters based on the features of the dataset."""

def churn_pred(data, crScore, geo, gender, age, tenure, balance, nop, hasCC, isAM, est_sal):
  geo_spain, geo_germany, gender_male = 0, 0, 0

  if geo == 'Germany':
    geo_germany = 1
  elif geo == 'Spain':
    geo_spain = 1

  if gender == 'Male':
    gender_male = 1

  data_points = pd.DataFrame(columns=data)
  vals = [crScore, age, tenure, balance, nop, hasCC, isAM, est_sal, geo_germany, geo_spain, gender_male]
  data_points.loc[len(data_points.index)] = vals

  scaler = StandardScaler()
  features = list(data)

  for col in features:
    data_points[col] = scaler.fit_transform(data_points[col].to_numpy().reshape(-1,1))

  predict = svc.predict(data_points)[0]

  if predict == 0:
    print(f"The Customer is more like to Stay")
  else:
    print(f"The Customer is more like to Exit")

data = X_train.columns
crScore = 815
geo = 'Spain'
gender = 'Female'
age = 39
tenure = 6
balance = 0
nop = 1
hasCC = 1
isAM = 1
est_sal = 85167.88

churn_pred(data, crScore, geo, gender, age, tenure, balance, nop, hasCC, isAM, est_sal)

data = X_train.columns
crScore = 654
geo = 'Germany'
gender = 'Male'
age = 36
tenure = 7
balance = 20000
nop = 2
hasCC = 1
isAM = 1
est_sal = 63521.95

churn_pred(data, crScore, geo, gender, age, tenure, balance, nop, hasCC, isAM, est_sal)